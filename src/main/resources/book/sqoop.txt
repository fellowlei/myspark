Sqoop教程
http://www.yiibai.com/sqoop/
http://blog.csdn.net/nsrainbow/article/details/41649671

sw
jdk-8u111-linux-x64.tar.gz
hadoop-2.6.4.tar.gz
sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
mysql-connector-java-5.1.40.tar.gz
mysql5.6

验证JAVA安装
java –version
验证Hadoop的安装
hadoop version

安装sqoop
    类似于jdk的安装
    下载sqoop(以sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz为例)
    解压下载的sqoop文件
    sudo tar -xzvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz  -C /usr/local
    重命名
    cd /usr/local
    sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop

    修改权限
    cd /usr/local
    sudo chown -R yourusername:yourusername sqoop

    添加环境变量
    sudo vim /etc/profile
    # 在最后添加下面内容
    export SQOOP_HOME=/usr/local/sqoop
    export PATH=$SQOOP_HOME/bin:$PATH

    使配置生效
    source /etc/profile

    配置Sqoop
    cd /usr/local/sqoop/conf
    mv sqoop-env-template.sh sqoop-env.sh

    配置sqoop-env.sh
    export HADOOP_COMMON_HOME=/usr/local/hadoop
    export HADOOP_MAPRED_HOME=/usr/local/hadoop

    下载并配置java的MySQL连接器
    #将mysql-connector-java-5.1.30-bin.jar拷贝到//usr/lib/sqoop/lib目录下
    mv mysql-connector-java-5.1.30-bin.jar /usr/lib/sqoop/lib

    验证Sqoop
    sqoop-version

#sqoop demo
#import from mysql to hdfs
sqoop import --connect jdbc:mysql://localhost:3306/spark --username root -P --table student --m 1
sqoop import --connect jdbc:mysql://localhost:3306/spark --table student  --username root -P --m 1 --where "age < 25" --target-dir /student
sqoop import --connect jdbc:mysql://localhost:3306/spark --username root -P --table student --m 1 --incremental append --check-column id --last-value 3

sqoop import-all-tables --connect jdbc:mysql://localhost:3306/spark --username root -P

sqoop export --connect jdbc:mysql://localhost:3306/spark --username root --P --table student --export-dir student
sqoop import-all-tables --connect jdbc:mysql://localhost:3306/spark --username root --P --m 1
sqoop-job --create myjob -- import --connect jdbc:mysql://localhost:3306/spark --username root -P --table student --m 1
sqoop codegen --connect jdbc:mysql://localhost:3306/spark --table student --username root -P
sqoop eval --connect jdbc:mysql://localhost:3306/spark --username root --query "select * from student where age < 25" -P
sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root -P
sqoop list-tables --connect jdbc:mysql://localhost:3306/spark --username root -P


# prepare mysql data
CREATE TABLE `employee` (
  `id` int(11) NOT NULL,
  `name` varchar(20) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM  DEFAULT CHARSET=utf8;

insert into employee (id,name) values (1,'michael');
insert into employee (id,name) values (2,'ted');
insert into employee (id,name) values (3,'jack');

CREATE TABLE `people` (
  `id` int(11) NOT NULL,
  `name` varchar(20) NOT NULL,
  `year` varchar(10),
  PRIMARY KEY (`id`)
) ENGINE=MyISAM  DEFAULT CHARSET=utf8;

insert into people values (1,'jack','2015');
insert into people values (2,'ted','2015');
insert into people values (3,'billy','2015');
insert into people values (4,'sara','2015');

#import from mysql to hive
sqoop import --connect jdbc:mysql://localhost:3306/spark --username root -P --table employee --hive-import --hive-table hive_employee --create-hive-table

sqoop import --connect jdbc:mysql://localhost:3306/spark --username root -P --query 'select id,name from people where year="2015" AND $CONDITIONS'  --direct -m 2 --split-by id --hive-import --create-hive-table --hive-table hive_people --target-dir /user/hive_people --hive-partition-key year --hive-partition-value '2015'

#import from mysql to hbase
sqoop import --connect jdbc:mysql://localhost:3306/spark --username root -P --table employee --hbase-table employee --column-family info --hbase-row-key id -m 1

