Ubuntu安装hive，并配置mysql作为元数据库
http://dblab.xmu.edu.cn/blog/install-hive/

sw
jdk-8u111-linux-x64.tar.gz
hadoop-2.6.4.tar.gz
apache-hive-1.2.1-bin.tar.gz
mysql-connector-java-5.1.40.tar.gz
mysql5.6

下载并解压hive源程序
    下载hive(以apache-hive-1.2.1-bin.tar.gz为例)
    #解压下载的hbase文件
    sudo tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /usr/local   # 解压到/usr/local中
    #重命名
    cd /usr/local/
    sudo mv apache-hive-1.2.1-bin hive       # 将文件夹名改为hive

    #修改权限
    cd /usr/local
    sudo chown -R yourusername:yourusername hive            # 修改文件权限

    #添加环境变量
    sudo vim /etc/profile
    # 在最后添加下面内容
    export HIVE_HOME=/usr/local/hive
    export PATH=$PATH:$HIVE_HOME/bin

    #使配置生效
    source /etc/profile

    修改/usr/local/hive/conf下的hive-site.xml
    将hive-default.xml.template重命名为hive-default.xml
    新建一个文件touch hive-site.xml，并在hive-site.xml中粘贴如下配置信息：
    <configuration>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
        <description>JDBC connect string for a JDBC metastore</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
        <description>username to use against metastore database</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
        <description>password to use against metastore database</description>
      </property>
    </configuration>

安装并配置mysql
    安装MySQL
    sudo apt-get update  #更新软件源
    sudo apt-get install mysql-server  #安装mysql
    启动和关闭mysql服务器
    service mysql start
    service mysql stop
    #ubuntu使用mysqld脚本启动：/etc/inint.d/mysql restart

    确认是否启动成功，mysql节点处于LISTEN状态表示启动成功：
    sudo netstat -tap | grep mysql

    进入mysql shell界面：
    mysql -u root -p

    解决利用sqoop导入MySQL中文乱码的问题（可以插入中文，但不能用sqoop导入中文）
    导致导入时中文乱码的原因是character_set_server默认设置是latin1
    建议按以下方式修改编码方式
    (1)编辑配置文件。sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf
    (2)在[mysqld]下添加一行character_set_server=utf8
    重启MySQL服务
    service mysql restart
    登陆MySQL，并查看MySQL目前设置的编码
    show variables like "char%";


下载mysql jdbc包
    #解压
    tar -zxvf mysql-connector-java-5.1.40.tar.gz
    #将mysql-connector-java-5.1.40-bin.jar拷贝到/usr/local/hive/lib目录下
    cp mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar  /usr/local/hive/lib
    启动并登陆mysql shell
    service mysql start #启动mysql服务
    mysql -u root -p  #登陆shell界面

    新建hive数据库
    #这个hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据
    create database hive;

    置mysql允许hive接入
    #将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码
    grant all on *.* to hive@localhost identified by 'hive';
    #刷新mysql系统权限关系表
    flush privileges;

    启动hive
    启动hive之前，请先启动hadoop集群
    start-all.sh #启动hadoop
    hive  #启动hive

    解决Hive启动，Hive metastore database is not initialized的错误。
    出错原因：重新安装Hive和MySQL，导致版本、配置不一致。在终端执行如下命令:
    schematool -dbType mysql -initSchema


==========================================
Hive四种数据导入方式
http://blog.csdn.net/lifuxiangcaohui/article/details/40588929

Hive的几种常见的数据导入方式
这里介绍四种：
（1）、从本地文件系统中导入数据到Hive表；
（2）、从HDFS上导入数据到Hive表；
（3）、从别的表中查询出相应的数据并导入到Hive表中；
（4）、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中。

一、从本地文件系统中导入数据到Hive表
hive里面创建好表
create table wyp (id int, name string, age int, tel string)
row format delimited
fields terminated by '\t'
stored as textfile

cat wyp.txt
1       wyp     25      13188888888888
2       test    30      13888888888888
3       zs      34      899314121

数据导入到wyp表
load data local inpath 'wyp.txt' into table wyp;

查看
select * from wyp;
dfs -ls /user/hive/warehouse/wyp;



二、HDFS上导入数据到Hive表
将数据临时复制到HDFS的一个目录
hadoop fs -put ./add.txt  ./

bin/hadoop fs -cat /user/root/add.txt
5   wyp1    23  131212121212
6   wyp2    24  134535353535
7   wyp3    25  132453535353
8   wyp4    26  154243434355

将这个文件里面的内容导入到Hive表中
load data inpath '/user/root/add.txt' into table wyp;

查看
select * from wyp;

三、从别的表中查询出相应的数据并导入到Hive表中
create table test (id int, name string, tel string) partitioned by (age int)
row format delimited
fields terminated by '\t'
stored as textfile
大体和wyp表的建表语句类似，只不过test表里面用age作为了分区字段

将wyp表中的查询结果并插入到test表中
hive> insert into table test
    > partition (age='25')
    > select id, name, tel
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略
#####################################################################
Total MapReduce CPU Time Spent: 1 seconds 310 msec
OK
Time taken: 19.125 seconds

查看
select * from wyp;


也可以在select语句里面通过使用分区值来动态指明分区
hive> set hive.exec.dynamic.partition.mode=nonstrict;
hive> insert into table test
    > partition (age)
    > select id, name,
    > tel, age
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略
#####################################################################
Total MapReduce CPU Time Spent: 1 seconds 510 msec
OK
Time taken: 17.712 seconds

查看
select * from wyp;

四、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中
hive> create table test4
    > as
    > select id, name, tel
    > from wyp;

hive> select * from test4;


===================================
Hive数据导出三种方式
http://blog.csdn.net/lifuxiangcaohui/article/details/40589881
根据导出的地方不一样，将这些方式分为三种：
（1）、导出到本地文件系统；
（2）、导出到HDFS中；
（3）、导出到Hive的另一个表中


一、导出到本地文件系统
 hive> insert overwrite local directory '/home/hadoop/result'
    > select * from wyp;
 这条HQL的执行需要启用Mapreduce完成


二、导出到HDFS中
 hive> insert overwrite directory '/user/root/hdfs/result'
    > select * from wyp;

三、导出到Hive的另一个表中
 hive> insert into table test
    > partition (age='25')
    > select id, name, tel
    > from wyp;
#####################################################################
           这里输出了一堆Mapreduce任务信息，这里省略


在Hive 0.11.0版本之间，数据的导出是不能指定列之间的分隔符的，只能用默认的列分隔符，也就是上面的^A来分割

指定输出结果列之间的分隔符
hive> insert overwrite local directory '/home/hadoop/result'
hive> row format delimited
hive> fields terminated by '\t'
hive> select * from test;



hive> insert overwrite local directory '/home/hadoop/result'
    > row format delimited
    > fields terminated by '\t'
    > select * from wyp;

我们还可以用hive的-e和-f参数来导出数据。
其中-e 表示后面直接接带双引号的sql语句；而-f是接一个文件，文件的内容为一个sql语句
hive -e "select * from wyp" >> local/wyp.txt

hive -f wyp.sql >> local/wyp2.txt


=========================================
 Hive总结（二）hive基本操作
 http://blog.csdn.net/lifuxiangcaohui/article/details/40261345

阅读本文章可以带着下面问题：
1.与传统数据库对比，找出他们的区别
2.熟练写出增删改查（面试必备）

创建表：
hive> CREATE TABLE pokes (foo INT, bar STRING);
        Creates a table called pokes with two columns, the first being an integer and the other a string

创建一个新表，结构与其他一样
hive> create table new_table like records;

创建分区表：
hive> create table logs(ts bigint,line string) partitioned by (dt String,country String);

加载分区表数据：
hive> load data local inpath '/home/hadoop/input/hive/partitions/file1' into table logs partition (dt='2001-01-01',country='GB');

展示表中有多少分区：
hive> show partitions logs;

展示所有表：
hive> SHOW TABLES;
        lists all the tables
hive> SHOW TABLES '.*s';

lists all the table that end with 's'. The pattern matching follows Java regular
expressions. Check out this link for documentation

显示表的结构信息
hive> DESCRIBE invites;
        shows the list of columns

更新表的名称：
hive> ALTER TABLE source RENAME TO target;

添加新一列
hive> ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');

删除表：
hive> DROP TABLE records;
删除表中数据，但要保持表的结构定义
hive> dfs -rmr /user/hive/warehouse/records;

从本地文件加载数据：
hive> LOAD DATA LOCAL INPATH '/home/hadoop/input/ncdc/micro-tab/sample.txt' OVERWRITE INTO TABLE records;

显示所有函数：
hive> show functions;

查看函数用法：
hive> describe function substr;

查看数组、map、结构
hive> select col1[0],col2['b'],col3.c from complex;


内连接：
hive> SELECT sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

查看hive为某个查询使用多少个MapReduce作业
hive> Explain SELECT sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

外连接：
hive> SELECT sales.*, things.* FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);
hive> SELECT sales.*, things.* FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);
hive> SELECT sales.*, things.* FROM sales FULL OUTER JOIN things ON (sales.id = things.id);

in查询：Hive不支持，但可以使用LEFT SEMI JOIN
hive> SELECT * FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);


Map连接：Hive可以把较小的表放入每个Mapper的内存来执行连接操作
hive> SELECT /*+ MAPJOIN(things) */ sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

INSERT OVERWRITE TABLE ..SELECT：新表预先存在
hive> FROM records2
    > INSERT OVERWRITE TABLE stations_by_year SELECT year, COUNT(DISTINCT station) GROUP BY year
    > INSERT OVERWRITE TABLE records_by_year SELECT year, COUNT(1) GROUP BY year
    > INSERT OVERWRITE TABLE good_records_by_year SELECT year, COUNT(1) WHERE temperature != 9999 AND (quality = 0 OR quality = 1 OR quality = 4 OR quality = 5 OR quality = 9) GROUP BY year;

CREATE TABLE ... AS SELECT：新表表预先不存在
hive>CREATE TABLE target AS SELECT col1,col2 FROM source;

创建视图：
hive> CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999;

查看视图详细信息：

Hive> DESCRIBE EXTENDED valid_records;
